{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HDF5 File Exploration with h5py\n\nThis notebook demonstrates how to open, navigate, and inspect HDF5 files commonly\nproduced at APS BER beamlines. We cover the essential h5py operations:\nopening files, listing groups, reading datasets, examining attributes, and\nunderstanding chunking and compression.\n\n**Prerequisites**: `pip install h5py numpy`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to an HDF5 file (update this to your local file)\n",
    "# Example files can be obtained from TomoBank: https://tomobank.readthedocs.io/\n",
    "FILEPATH = \"sample_data.h5\"\n",
    "\n",
    "print(f\"h5py version: {h5py.__version__}\")\n",
    "print(f\"HDF5 version: {h5py.version.hdf5_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening an HDF5 file and printing the full tree structure\n",
    "\n",
    "def print_hdf5_tree(filepath):\n",
    "    \"\"\"Print the complete tree structure of an HDF5 file.\"\"\"\n",
    "    with h5py.File(filepath, \"r\") as f:\n",
    "        print(f\"File: {filepath}\")\n",
    "        print(f\"File size: {Path(filepath).stat().st_size / 1e6:.1f} MB\")\n",
    "        print(f\"Root groups: {list(f.keys())}\")\n",
    "        print(\"\\nFull tree:\")\n",
    "        print(\"â”€\" * 60)\n",
    "        \n",
    "        def visitor(name, obj):\n",
    "            indent = \"  \" * name.count(\"/\")\n",
    "            if isinstance(obj, h5py.Group):\n",
    "                n_attrs = len(obj.attrs)\n",
    "                attr_str = f\"  ({n_attrs} attrs)\" if n_attrs > 0 else \"\"\n",
    "                print(f\"{indent}[G] {name.split('/')[-1]}/{attr_str}\")\n",
    "            elif isinstance(obj, h5py.Dataset):\n",
    "                shape_str = str(obj.shape)\n",
    "                dtype_str = str(obj.dtype)\n",
    "                size_mb = obj.nbytes / 1e6\n",
    "                print(f\"{indent}[D] {name.split('/')[-1]}  {shape_str}  {dtype_str}  ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        f.visititems(visitor)\n",
    "\n",
    "print_hdf5_tree(FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigating groups and accessing datasets\n",
    "\n",
    "with h5py.File(FILEPATH, \"r\") as f:\n",
    "    # Navigate using dictionary-like syntax\n",
    "    root_keys = list(f.keys())\n",
    "    print(f\"Root-level entries: {root_keys}\")\n",
    "    \n",
    "    # Access a group (adjust path based on your file)\n",
    "    # Common paths: /exchange, /MAPS, /entry\n",
    "    for key in root_keys:\n",
    "        obj = f[key]\n",
    "        if isinstance(obj, h5py.Group):\n",
    "            print(f\"\\nGroup '/{key}/' contains: {list(obj.keys())}\")\n",
    "        elif isinstance(obj, h5py.Dataset):\n",
    "            print(f\"\\nDataset '/{key}': shape={obj.shape}, dtype={obj.dtype}\")\n",
    "    \n",
    "    # Read a dataset (lazy -- no data loaded until sliced)\n",
    "    # Example: reading the first dataset found\n",
    "    first_dataset_path = None\n",
    "    def find_first_dataset(name, obj):\n",
    "        global first_dataset_path\n",
    "        if isinstance(obj, h5py.Dataset) and first_dataset_path is None:\n",
    "            first_dataset_path = name\n",
    "    f.visititems(find_first_dataset)\n",
    "    \n",
    "    if first_dataset_path:\n",
    "        dset = f[first_dataset_path]\n",
    "        print(f\"\\nFirst dataset: /{first_dataset_path}\")\n",
    "        print(f\"  Shape: {dset.shape}\")\n",
    "        print(f\"  Dtype: {dset.dtype}\")\n",
    "        print(f\"  Chunks: {dset.chunks}\")\n",
    "        print(f\"  Compression: {dset.compression}\")\n",
    "        print(f\"  Compression opts: {dset.compression_opts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and inspecting attributes\n",
    "\n",
    "with h5py.File(FILEPATH, \"r\") as f:\n",
    "    \n",
    "    def print_all_attributes(name, obj):\n",
    "        \"\"\"Print attributes attached to every group and dataset.\"\"\"\n",
    "        if len(obj.attrs) > 0:\n",
    "            print(f\"\\n/{name}\")\n",
    "            for attr_name, attr_value in obj.attrs.items():\n",
    "                # Handle byte strings\n",
    "                if isinstance(attr_value, bytes):\n",
    "                    attr_value = attr_value.decode(\"utf-8\", errors=\"replace\")\n",
    "                elif isinstance(attr_value, np.ndarray) and attr_value.dtype.kind == 'S':\n",
    "                    attr_value = [v.decode() for v in attr_value]\n",
    "                print(f\"  @{attr_name} = {attr_value}\")\n",
    "    \n",
    "    # Root-level attributes\n",
    "    if len(f.attrs) > 0:\n",
    "        print(\"/  (root attributes)\")\n",
    "        for k, v in f.attrs.items():\n",
    "            print(f\"  @{k} = {v}\")\n",
    "    \n",
    "    f.visititems(print_all_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data: full load vs. partial (slice) access\n",
    "\n",
    "with h5py.File(FILEPATH, \"r\") as f:\n",
    "    # Find a multi-dimensional dataset\n",
    "    target = None\n",
    "    def find_nd_dataset(name, obj):\n",
    "        nonlocal target\n",
    "        if isinstance(obj, h5py.Dataset) and len(obj.shape) >= 2 and target is None:\n",
    "            target = name\n",
    "    f.visititems(find_nd_dataset)\n",
    "    \n",
    "    if target:\n",
    "        dset = f[target]\n",
    "        print(f\"Dataset: /{target}, shape={dset.shape}, size={dset.nbytes/1e6:.1f} MB\")\n",
    "        \n",
    "        # Method 1: Load entire dataset into memory\n",
    "        if dset.nbytes < 100e6:  # Only if < 100 MB\n",
    "            full_data = dset[:]\n",
    "            print(f\"  Full load: {full_data.shape}, {full_data.nbytes/1e6:.1f} MB\")\n",
    "        \n",
    "        # Method 2: Load a single slice (much more memory efficient)\n",
    "        if len(dset.shape) >= 2:\n",
    "            single_frame = dset[0]  # First frame along axis 0\n",
    "            print(f\"  Single frame: {single_frame.shape}\")\n",
    "        \n",
    "        # Method 3: Load a region of interest\n",
    "        if len(dset.shape) >= 2:\n",
    "            h, w = dset.shape[-2], dset.shape[-1]\n",
    "            roi = dset[..., h//4:3*h//4, w//4:3*w//4]  # Central 50%\n",
    "            print(f\"  Central ROI: {roi.shape}\")\n",
    "        \n",
    "        # Compute statistics without loading everything\n",
    "        print(f\"\\n  Statistics (first frame):\")\n",
    "        frame = dset[0] if len(dset.shape) >= 2 else dset[:]\n",
    "        print(f\"    Min:    {np.nanmin(frame)}\")\n",
    "        print(f\"    Max:    {np.nanmax(frame)}\")\n",
    "        print(f\"    Mean:   {np.nanmean(frame):.4f}\")\n",
    "        print(f\"    Std:    {np.nanstd(frame):.4f}\")\n",
    "        print(f\"    NaNs:   {np.isnan(frame).sum() if np.issubdtype(frame.dtype, np.floating) else 'N/A'}\")\n",
    "    else:\n",
    "        print(\"No multi-dimensional dataset found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding chunking and compression\n",
    "\n",
    "with h5py.File(FILEPATH, \"r\") as f:\n",
    "    print(\"Dataset storage analysis:\")\n",
    "    print(f\"{'Path':<50s} {'Shape':<20s} {'Chunks':<20s} {'Compression':<12s} {'Ratio':<8s}\")\n",
    "    print(\"=\" * 110)\n",
    "    \n",
    "    def analyze_storage(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset) and obj.nbytes > 0:\n",
    "            raw_size = obj.nbytes\n",
    "            # Estimate stored size from id.get_storage_size()\n",
    "            stored_size = obj.id.get_storage_size()\n",
    "            ratio = raw_size / max(stored_size, 1)\n",
    "            \n",
    "            comp = obj.compression if obj.compression else \"none\"\n",
    "            chunks = str(obj.chunks) if obj.chunks else \"contiguous\"\n",
    "            \n",
    "            print(f\"  /{name:<48s} {str(obj.shape):<20s} {chunks:<20s} {comp:<12s} {ratio:<.1f}x\")\n",
    "    \n",
    "    f.visititems(analyze_storage)\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"  - Ratio > 1 means data is compressed (higher = more compression)\")\n",
    "print(\"  - Chunked datasets allow efficient partial reads\")\n",
    "print(\"  - Chunk shape should align with typical access patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key h5py operations covered in this notebook:\n",
    "\n",
    "| Operation | Code |\n",
    "|-----------|------|\n",
    "| Open file | `h5py.File(path, \"r\")` |\n",
    "| List groups | `list(f.keys())` or `list(f[\"group\"].keys())` |\n",
    "| Walk tree | `f.visititems(callback)` |\n",
    "| Read dataset | `data = f[\"/path/to/dataset\"][:]` |\n",
    "| Slice dataset | `frame = f[\"/path/to/dataset\"][0, :, :]` |\n",
    "| Read attribute | `val = f[\"/path\"].attrs[\"name\"]` |\n",
    "| Check chunks | `f[\"/path/to/dataset\"].chunks` |\n",
    "| Check compression | `f[\"/path/to/dataset\"].compression` |\n",
    "\n",
    "**Next**: See [02_data_visualization.ipynb](02_data_visualization.ipynb) for visualizing\n",
    "synchrotron data arrays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}